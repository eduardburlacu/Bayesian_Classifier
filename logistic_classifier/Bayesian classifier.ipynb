{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T14:00:57.073072100Z",
     "start_time": "2024-03-08T14:00:57.044976400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X = np.loadtxt('X.txt')\n",
    "y = np.loadtxt('y.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T14:00:57.694695100Z",
     "start_time": "2024-03-08T14:00:57.635900300Z"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# X: 2d array with the input features\n",
    "# y: 1d array with the class labels (0 or 1)\n",
    "#\n",
    "def plot_data_internal(X, y,title=''):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    ax = plt.gca()\n",
    "    ax.plot(X[y == 0, 0], X[y == 0, 1], 'ro', markersize=2, label = 'Class 1')\n",
    "    ax.plot(X[y == 1, 0], X[y == 1, 1], 'bo', markersize=2, label = 'Class 2')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title('Plot data '+title)\n",
    "    plt.legend(loc = 'upper left', scatterpoints = 1, numpoints = 1)\n",
    "    return xx, yy\n",
    "\n",
    "##\n",
    "# X: 2d array with the input features\n",
    "# y: 1d array with the class labels (0 or 1)\n",
    "#\n",
    "def plot_data(X, y):\n",
    "    xx, yy = plot_data_internal(X, y)\n",
    "    plt.show()\n",
    "\n",
    "##\n",
    "# x: input to the logistic function\n",
    "#\n",
    "def logistic(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "##\n",
    "# X: 2d array with the input features\n",
    "# y: 1d array with the class labels (0 or 1)\n",
    "# w: current parameter values\n",
    "#\n",
    "def compute_average_ll(X, y, w):\n",
    "    output_prob = logistic(np.dot(X, w))\n",
    "    return np.mean(y * np.log(output_prob)\n",
    "                   + (1 - y) * np.log(1.0 - output_prob))\n",
    "\n",
    "def compute_average_ll_fixed(X, y, w):\n",
    "    return np.mean(y * np.log(logistic(np.dot(X, w)))\n",
    "                   + (1 - y) * np.log(logistic(-np.dot(X, w))))\n",
    "\n",
    "##\n",
    "# ll: 1d array with the average likelihood per data point, for each training\n",
    "# step. The dimension of this array should be equal to the number of training\n",
    "# steps.\n",
    "#\n",
    "def plot_ll(ll):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.xlim(0, len(ll) + 2)\n",
    "    plt.ylim(min(ll) - 0.1, max(ll) + 0.1)\n",
    "    ax.plot(np.arange(1, len(ll) + 1), ll, 'r-')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average log-likelihood')\n",
    "    plt.title('Plot Average Log-likelihood Curve')\n",
    "    plt.show()\n",
    "\n",
    "##\n",
    "# X: 2d array with input features at which to compute predictions.\n",
    "#(uses parameter vector w which is defined outside the function's scope)\n",
    "#\n",
    "def predict_for_plot(x):\n",
    "    x_tilde = np.concatenate((x, np.ones((x.shape[ 0 ], 1 ))), 1)\n",
    "    return logistic(np.dot(x_tilde, w))\n",
    "\n",
    "##\n",
    "# X: 2d array with the input features\n",
    "# y: 1d array with the class labels (0 or 1)\n",
    "# predict: function that recives as input a feature matrix and returns a 1d\n",
    "#          vector with the probability of class 1.\n",
    "def plot_predictive_distribution(X, y, predict,title):\n",
    "    xx, yy = plot_data_internal(X, y,title)\n",
    "    ax = plt.gca()\n",
    "    X_predict = np.concatenate((xx.ravel().reshape((-1, 1)),\n",
    "                                yy.ravel().reshape((-1, 1))), 1)\n",
    "    Z = predict(X_predict)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs2 = ax.contour(xx, yy, Z, cmap = 'RdBu', linewidths = 2, levels=[0.1,0.3,0.5,0.7,0.9])\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize = 14)\n",
    "    plt.imshow(Z,interpolation=\"bilinear\", origin=\"lower\", cmap=\"RdBu\", extent=(np.amin(xx), np.amax(xx), np.amin(yy), np.amax(yy)), zorder=0)\n",
    "    plt.show()\n",
    "\n",
    "##\n",
    "# l: hyper-parameter for the width of the Gaussian basis functions\n",
    "# Z: location of the Gaussian basis functions\n",
    "# X: points at which to evaluate the basis functions\n",
    "def expand_inputs(l, X, Z):\n",
    "    X2 = np.sum(X**2, 1)\n",
    "    Z2 = np.sum(Z**2, 1)\n",
    "    ones_Z = np.ones(Z.shape[ 0 ])\n",
    "    ones_X = np.ones(X.shape[ 0 ])\n",
    "    r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)\n",
    "    return np.exp(-0.5 / l**2 * r2)\n",
    "\n",
    "##\n",
    "# x: 2d array with input features at which to compute the predictions\n",
    "# using the feature expansion\n",
    "#\n",
    "# (uses parameter vector w and the 2d array X with the centers of the basis\n",
    "# functions for the feature expansion, which are defined outside the function's\n",
    "# scope)\n",
    "#\n",
    "def predict_for_plot_expanded_features(x):\n",
    "    x_expanded = expand_inputs(l, x, C)\n",
    "    x_tilde = np.concatenate((x_expanded, np.ones((x_expanded.shape[ 0 ], 1 ))), 1)\n",
    "    return logistic(np.dot(x_tilde, w_MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T14:00:59.042407100Z",
     "start_time": "2024-03-08T14:00:58.994679200Z"
    }
   },
   "outputs": [],
   "source": [
    "def neg_log_posterior_not_normalized(w):\n",
    "    output_prob = logistic(np.dot(X_train, w))\n",
    "    log_likelihood = np.dot(y_train, np.log(output_prob)) + np.dot((1 - y_train),np.log(1.0 - output_prob))\n",
    "    log_prior_without_const = -0.5 * np.dot(w,w)/prior_var\n",
    "    return -(log_likelihood + log_prior_without_const)\n",
    "\n",
    "def grad_neg_log_posterior(w):\n",
    "    output_prob = logistic(np.dot(X_train, w))\n",
    "    grad_log_prior = -w/(prior_var)\n",
    "    grad_log_likelihood = np.dot(y_train-output_prob.T, X_train)\n",
    "    return -(grad_log_prior+grad_log_likelihood)\n",
    "\n",
    "def predict_for_full_bayesian(x):\n",
    "    results = np.zeros(x.shape[0])\n",
    "    x_expanded = expand_inputs(l, x, C)\n",
    "    x_tilde = np.concatenate((x_expanded, np.ones((x_expanded.shape[ 0 ], 1 ))), 1)\n",
    "#     print(x_tilde.shape)\n",
    "#     print(x_tilde[0].shape)\n",
    "    for i in range(x.shape[0]):\n",
    "        mu_a = np.dot(w_MAP, np.transpose(x_tilde[i]))\n",
    "    \n",
    "\n",
    "        sigma_a = np.dot(x_tilde[i],np.dot(S_posterior,np.transpose(x_tilde[i])))\n",
    "\n",
    "        kappa = (1 + np.pi*sigma_a/8)**-0.5\n",
    "        results[i] = logistic(np.dot(kappa,mu_a))\n",
    "    return results\n",
    "\n",
    "def compute_average_lme(X,y,w_MAP,S_posterior,prior_var):\n",
    "    log_likelihood = ( y * np.log(logistic(np.dot(X, w_MAP)))\n",
    "                   + (1 - y) * np.log(logistic(-np.dot(X, w_MAP))))\n",
    "    print('log likelihood is', np.sum(log_likelihood))\n",
    "    \n",
    "    occam_1 = -0.5*np.dot(w_MAP,w_MAP)/(prior_var)\n",
    "    print('occam 1 is', occam_1)\n",
    "    \n",
    "    occam_2 = -0.5*w_MAP.size*np.log(prior_var)\n",
    "    print('occam 2 is', occam_2)\n",
    "    \n",
    "    occam_3 = 0.5*np.linalg.slogdet(S_posterior)[1]\n",
    "    print('occam 3 is', occam_3)\n",
    "    \n",
    "    print('log model evidence is', np.sum(log_likelihood)+occam_1+occam_2+occam_3)\n",
    "    return np.sum(log_likelihood)+occam_1+occam_2+occam_3\n",
    "\n",
    "def compute_average_bayes_ll(X, y, w,S):\n",
    "    output_prob = np.zeros(y.size)\n",
    "    for i in range(len(output_prob)):\n",
    "        mu_a = np.dot(w, X[i,:])\n",
    "    \n",
    "        sigma_a = np.dot(X[i,:],np.dot(S,X[i,:]))\n",
    "\n",
    "        kappa = (1 + np.pi*sigma_a/8)**-0.5\n",
    "        \n",
    "        output_prob[i] = logistic(np.dot(kappa,mu_a))\n",
    "    results = y*np.log(output_prob) + (1-y)*np.log(output_prob)\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T14:01:01.727044Z",
     "start_time": "2024-03-08T14:01:01.704239200Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_conditional_posterior():\n",
    "    w_for_plot = np.zeros([100,w_MAP.size])\n",
    "    count = 0\n",
    "    for i in np.linspace(-1,1,100):\n",
    "        w_for_plot[count] = np.concatenate([np.array([i]),w_MAP[1:]])\n",
    "        count += 1\n",
    "    prob_for_plot = [np.exp(log_posterior(w_for_plot[i])) for i in range(len(w_for_plot))]\n",
    "    prob_for_plot = np.array(prob_for_plot)\n",
    "\n",
    "    plt.plot(np.linspace(-1,1,100),prob_for_plot)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on l = 0.1 and variance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T14:01:52.159345700Z",
     "start_time": "2024-03-08T14:01:03.736380800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 0\n",
      "## l   = 0.1\n",
      "## var = 1\n",
      "___FULL BAYESIAN___\n",
      "TRAIN LL: -1.0041454429145582\n",
      "TEST LL: -0.9292464317323779\n",
      "\n",
      "___MAP___\n",
      "TRAIN LL: -0.21659107076658657\n",
      "TEST LL: -0.3242690355101158\n",
      "\n",
      "___TEST MODEL EVIDENCE___:\n",
      "log likelihood is -64.85380710202317\n",
      "occam 1 is -65.88356907242733\n",
      "occam 2 is -0.0\n",
      "occam 3 is -78.01973970579348\n",
      "log model evidence is -208.757115880244\n",
      "===FB CONFUSION MATRIX===\n",
      "0.9108910891089109 0.0891089108910891\n",
      "0.1414141414141414 0.8585858585858586\n",
      "===CONFUSION MATRIX===\n",
      "0.9108910891089109 0.0891089108910891\n",
      "0.1414141414141414 0.8585858585858586\n",
      "______________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ls = np.array([0.1])\n",
    "variances = np.array([1])\n",
    "\n",
    "grid5 = np.zeros([ls.size,variances.size])\n",
    "\n",
    "for l_index in range(ls.size):\n",
    "    for var_index in range(variances.size):\n",
    "        print(\"RUN\", l_index*variances.size + var_index)\n",
    "        #Prior hyperparameters\n",
    "        prior_mean = 0\n",
    "        prior_var = variances[var_index]\n",
    "\n",
    "        #RBF width\n",
    "        l = ls[l_index]        \n",
    "        \n",
    "        print('## l   =',l)\n",
    "        print('## var =',prior_var)\n",
    "        \n",
    "        #Train test split\n",
    "        train_portion = 0.8\n",
    "        train_size = int(np.size(X,0)*train_portion)\n",
    "\n",
    "        #RBF centres\n",
    "        C = X[:train_size,:]\n",
    "\n",
    "        # #RBF train set\n",
    "        X_train = expand_inputs(l,X[:train_size,:],C)\n",
    "        X_train = np.column_stack([X_train,np.ones(X_train.shape[0])])\n",
    "        y_train = y[:train_size]\n",
    "\n",
    "        # #RBF test set\n",
    "        X_test = expand_inputs(l,X[train_size:,:],C)\n",
    "        X_test = np.column_stack([X_test,np.ones(X_test.shape[0])])\n",
    "        y_test = y[train_size:]\n",
    "\n",
    "        #Initialise Gaussian prior\n",
    "        np.random.seed(42)\n",
    "        w_prior = np.random.normal(prior_mean,prior_var**0.5,X_train.shape[1])\n",
    "        w = w_prior\n",
    "        S_prior = prior_var * np.eye(w.size)\n",
    "\n",
    "        #MAP with bfgs\n",
    "        w_MAP = optimize.fmin_l_bfgs_b(neg_log_posterior_not_normalized, w_prior, fprime=grad_neg_log_posterior)[0]\n",
    "        #Calculate S_posterior\n",
    "        Hessian =  np.linalg.inv(S_prior)\n",
    "        Hessian += sum(logistic(np.dot(X_train[i],w_MAP)) * (1-logistic(np.dot(X_train[i],w_MAP))) * np.outer(X_train[i],X_train[i]) for i in range(X_train.shape[0]))\n",
    "        S_posterior = np.linalg.inv(Hessian)\n",
    "\n",
    "    #         if np.all(np.linalg.eigvals(S_posterior) > 0): print('S_posterior is positive definite')\n",
    "\n",
    "        plot_predictive_distribution(X,y,predict_for_full_bayesian,'Full Bayesian')\n",
    "        plot_predictive_distribution(X,y,predict_for_plot_expanded_features,'MAP')\n",
    "\n",
    "        print('___FULL BAYESIAN___')\n",
    "        print('TRAIN LL:',compute_average_bayes_ll(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_bayes_ll(X_test,y_test,w_MAP,S_posterior))\n",
    "\n",
    "        print('\\n___MAP___')\n",
    "        print('TRAIN LL:',compute_average_ll_fixed(X_train,y_train,w_MAP))\n",
    "    #         print('TRAIN LOG MODEL EVIDENCE:',compute_average_lme(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_ll_fixed(X_test,y_test,w_MAP))\n",
    "    #         print('TEST LOG MODEL EVIDENCE:',compute_average_lme(X_test,y_test,w_MAP,S_posterior))\n",
    "\n",
    "        print('\\n___TEST MODEL EVIDENCE___:')\n",
    "        compute_average_lme(X_test,y_test,w_MAP,S_posterior,prior_var)\n",
    "\n",
    "        TP, TN, FP, FN = 0,0,0,0\n",
    "        for i in range(len(X_test)):\n",
    "            mu_a = np.dot(w_MAP, X_test[i,:])\n",
    "            sigma_a = np.dot(X_test[i,:],np.dot(S_posterior,X_test[i,:]))\n",
    "            kappa = (1 + np.pi*sigma_a/8)**-0.5\n",
    "\n",
    "            prediction = 1 if logistic(kappa*mu_a)>0.5 else 0\n",
    "            if prediction == y_test[i]:\n",
    "                if y_test[i]==0: TN += 1\n",
    "                elif y_test[i]==1: TP += 1\n",
    "            else:\n",
    "                if y_test[i]==0: FP += 1\n",
    "                elif y_test[i]==1: FN += 1\n",
    "        print('===FB CONFUSION MATRIX===')\n",
    "        print(TN/(TN+FP), FP/(TN+FP))\n",
    "        print(FN/(FN+TP), TP/(FN+TP))\n",
    "        \n",
    "        \n",
    "        TP, TN, FP, FN = 0,0,0,0\n",
    "        for i in range(len(X_test)):\n",
    "            prediction = 1 if logistic(np.dot(X_test[i],w_MAP))>0.5 else 0\n",
    "            if prediction == y_test[i]:\n",
    "                if y_test[i]==0: TN += 1\n",
    "                elif y_test[i]==1: TP += 1\n",
    "            else:\n",
    "                if y_test[i]==0: FP += 1\n",
    "                elif y_test[i]==1: FN += 1\n",
    "        print('===CONFUSION MATRIX===')\n",
    "        print(TN/(TN+FP), FP/(TN+FP))\n",
    "        print(FN/(FN+TP), TP/(FN+TP))\n",
    "\n",
    "        print(\"______________________________\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.967302Z"
    }
   },
   "outputs": [],
   "source": [
    "# ls = np.linspace(0.01,10,5)\n",
    "# variances = np.linspace(.1,10000,5)\n",
    "ls = np.array([0.01,0.1,1,10])\n",
    "variances = np.array([0.1,1,10,100,1000,10000])\n",
    "\n",
    "grid1 = np.zeros([ls.size,variances.size])\n",
    "\n",
    "for l_index in range(ls.size):\n",
    "    for var_index in range(variances.size):\n",
    "        print(\"RUN\", l_index*variances.size + var_index)\n",
    "        #Prior hyperparameters\n",
    "        prior_mean = 0\n",
    "        prior_var = variances[var_index]\n",
    "\n",
    "        #RBF width\n",
    "        l = ls[l_index]        \n",
    "        \n",
    "        print('## l   =',l)\n",
    "        print('## var =',prior_var)\n",
    "        \n",
    "        #Train test split\n",
    "        train_portion = 0.8\n",
    "        train_size = int(np.size(X,0)*train_portion)\n",
    "\n",
    "        #RBF centres\n",
    "        C = X[:train_size,:]\n",
    "\n",
    "        # #RBF train set\n",
    "        X_train = expand_inputs(l,X[:train_size,:],C)\n",
    "        X_train = np.column_stack([X_train,np.ones(X_train.shape[0])])\n",
    "        y_train = y[:train_size]\n",
    "\n",
    "        # #RBF test set\n",
    "        X_test = expand_inputs(l,X[train_size:,:],C)\n",
    "        X_test = np.column_stack([X_test,np.ones(X_test.shape[0])])\n",
    "        y_test = y[train_size:]\n",
    "        \n",
    "        #Initialise Gaussian prior\n",
    "        np.random.seed(42)\n",
    "        w_prior = np.random.normal(prior_mean,prior_var**0.5,X_train.shape[1])\n",
    "        w = w_prior\n",
    "        S_prior = prior_var * np.eye(w.size)\n",
    "\n",
    "        #MAP with bfgs\n",
    "        w_MAP = optimize.fmin_l_bfgs_b(neg_log_posterior_not_normalized, w_prior, fprime=grad_neg_log_posterior)[0]\n",
    "        #Calculate S_posterior\n",
    "        Hessian =  np.linalg.inv(S_prior)\n",
    "        Hessian += sum(logistic(np.dot(X_train[i],w_MAP)) * (1-logistic(np.dot(X_train[i],w_MAP))) * np.outer(X_train[i],X_train[i]) for i in range(X_train.shape[0]))\n",
    "        S_posterior = np.linalg.inv(Hessian)\n",
    "\n",
    "#         if np.all(np.linalg.eigvals(S_posterior) > 0): print('S_posterior is positive definite')\n",
    "        \n",
    "        plot_predictive_distribution(X,y,predict_for_full_bayesian,'Full Bayesian')\n",
    "        plot_predictive_distribution(X,y,predict_for_plot_expanded_features,'MAP')\n",
    "        \n",
    "        print('___FULL BAYESIAN___')\n",
    "        print('TRAIN LL:',compute_average_bayes_ll(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_bayes_ll(X_test,y_test,w_MAP,S_posterior))\n",
    "        \n",
    "        print('\\n___MAP___')\n",
    "        print('TRAIN LL:',compute_average_ll_fixed(X_train,y_train,w_MAP))\n",
    "#         print('TRAIN LOG MODEL EVIDENCE:',compute_average_lme(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_ll_fixed(X_test,y_test,w_MAP))\n",
    "#         print('TEST LOG MODEL EVIDENCE:',compute_average_lme(X_test,y_test,w_MAP,S_posterior))\n",
    "\n",
    "        print('\\n___TEST MODEL EVIDENCE___:')\n",
    "        grid1[l_index,var_index] = compute_average_lme(X_test,y_test,w_MAP,S_posterior,prior_var)\n",
    "        \n",
    "        TP, TN, FP, FN = 0,0,0,0\n",
    "        for i in range(len(X_test)):\n",
    "            mu_a = np.dot(w_MAP, X_test[i,:])\n",
    "            sigma_a = np.dot(X_test[i,:],np.dot(S_posterior,X_test[i,:]))\n",
    "            kappa = (1 + np.pi*sigma_a/8)**-0.5\n",
    "            \n",
    "            prediction = 1 if logistic(kappa*mu_a)>0.5 else 0\n",
    "            if prediction == y_test[i]:\n",
    "                if y_test[i]==0: TN += 1\n",
    "                elif y_test[i]==1: TP += 1\n",
    "            else:\n",
    "                if y_test[i]==0: FP += 1\n",
    "                elif y_test[i]==1: FN += 1\n",
    "        print('===CONFUSION MATRIX===')\n",
    "        print(TN/(TN+FP), FP/(TN+FP))\n",
    "        print(FN/(FN+TP), TP/(FN+TP))\n",
    "        \n",
    "        print(\"______________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.967302Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_df = pd.DataFrame(data = np.round(grid1,2), index=ls, columns=variances)\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.970868700Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_df.to_latex('grid1.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.971882700Z"
    }
   },
   "outputs": [],
   "source": [
    "grid1.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.972876200Z"
    }
   },
   "outputs": [],
   "source": [
    "ls = np.linspace(0.1,10,10)\n",
    "variances = np.linspace(1,100,10)\n",
    "# ls = np.array([0.1,1,10])\n",
    "# variances = np.array([0.1,1,10])\n",
    "\n",
    "grid2 = np.zeros([ls.size,variances.size])\n",
    "\n",
    "for l_index in range(ls.size):\n",
    "    for var_index in range(variances.size):\n",
    "        print(\"RUN\", l_index*variances.size + var_index)\n",
    "        #Prior hyperparameters\n",
    "        prior_mean = 0\n",
    "        prior_var = variances[var_index]\n",
    "\n",
    "        #RBF width\n",
    "        l = ls[l_index]        \n",
    "        \n",
    "        print('## l   =',l)\n",
    "        print('## var =',prior_var)\n",
    "        \n",
    "        #Train test split\n",
    "        train_portion = 0.8\n",
    "        train_size = int(np.size(X,0)*train_portion)\n",
    "\n",
    "        #RBF centres\n",
    "        C = X[:train_size,:]\n",
    "\n",
    "        # #RBF train set\n",
    "        X_train = expand_inputs(l,X[:train_size,:],C)\n",
    "        X_train = np.column_stack([X_train,np.ones(X_train.shape[0])])\n",
    "        y_train = y[:train_size]\n",
    "\n",
    "        # #RBF test set\n",
    "        X_test = expand_inputs(l,X[train_size:,:],C)\n",
    "        X_test = np.column_stack([X_test,np.ones(X_test.shape[0])])\n",
    "        y_test = y[train_size:]\n",
    "        \n",
    "        #Initialise Gaussian prior\n",
    "        np.random.seed(42)\n",
    "        w_prior = np.random.normal(prior_mean,prior_var**0.5,X_train.shape[1])\n",
    "        w = w_prior\n",
    "        S_prior = prior_var * np.eye(w.size)\n",
    "\n",
    "        #MAP with bfgs\n",
    "        w_MAP = optimize.fmin_l_bfgs_b(neg_log_posterior_not_normalized, w_prior, fprime=grad_neg_log_posterior)[0]\n",
    "        #Calculate S_posterior\n",
    "        Hessian =  np.linalg.inv(S_prior)\n",
    "        Hessian += sum(logistic(np.dot(X_train[i],w_MAP)) * (1-logistic(np.dot(X_train[i],w_MAP))) * np.outer(X_train[i],X_train[i]) for i in range(X_train.shape[0]))\n",
    "        S_posterior = np.linalg.inv(Hessian)\n",
    "\n",
    "#         if np.all(np.linalg.eigvals(S_posterior) > 0): print('S_posterior is positive definite')\n",
    "        \n",
    "        plot_predictive_distribution(X,y,predict_for_full_bayesian,'Full Bayesian')\n",
    "        plot_predictive_distribution(X,y,predict_for_plot_expanded_features,'MAP')\n",
    "        \n",
    "        print('___FULL BAYESIAN___')\n",
    "        print('TRAIN LL:',compute_average_bayes_ll(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_bayes_ll(X_test,y_test,w_MAP,S_posterior))\n",
    "        \n",
    "        print('\\n___MAP___')\n",
    "        print('TRAIN LL:',compute_average_ll_fixed(X_train,y_train,w_MAP))\n",
    "#         print('TRAIN LOG MODEL EVIDENCE:',compute_average_lme(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_ll_fixed(X_test,y_test,w_MAP))\n",
    "#         print('TEST LOG MODEL EVIDENCE:',compute_average_lme(X_test,y_test,w_MAP,S_posterior))\n",
    "\n",
    "        print('\\n___TEST MODEL EVIDENCE___:')\n",
    "        grid2[l_index,var_index] = compute_average_lme(X_test,y_test,w_MAP,S_posterior,prior_var)\n",
    "        \n",
    "        TP, TN, FP, FN = 0,0,0,0\n",
    "        for i in range(len(X_test)):\n",
    "            mu_a = np.dot(w_MAP, X_test[i,:])\n",
    "            sigma_a = np.dot(X_test[i,:],np.dot(S_posterior,X_test[i,:]))\n",
    "            kappa = (1 + np.pi*sigma_a/8)**-0.5\n",
    "            \n",
    "            prediction = 1 if logistic(kappa*mu_a)>0.5 else 0\n",
    "            if prediction == y_test[i]:\n",
    "                if y_test[i]==0: TN += 1\n",
    "                elif y_test[i]==1: TP += 1\n",
    "            else:\n",
    "                if y_test[i]==0: FP += 1\n",
    "                elif y_test[i]==1: FN += 1\n",
    "        print('===CONFUSION MATRIX===')\n",
    "        print(TN/(TN+FP), FP/(TN+FP))\n",
    "        print(FN/(FN+TP), TP/(FN+TP))\n",
    "        \n",
    "        print(\"______________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.975874400Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_df = pd.DataFrame(data = np.round(grid2,2), index=ls, columns=variances)\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.977378800Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_df.to_latex('grid2.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.978385500Z"
    }
   },
   "outputs": [],
   "source": [
    "grid2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.979385500Z"
    }
   },
   "outputs": [],
   "source": [
    "ls = np.linspace(0.1,2.3,10)\n",
    "variances = np.linspace(1,23,10)\n",
    "# ls = np.array([0.1,1,10])\n",
    "# variances = np.array([0.1,1,10])\n",
    "\n",
    "grid3 = np.zeros([ls.size,variances.size])\n",
    "\n",
    "for l_index in range(ls.size):\n",
    "    for var_index in range(variances.size):\n",
    "        print(\"RUN\", l_index*variances.size + var_index)\n",
    "        #Prior hyperparameters\n",
    "        prior_mean = 0\n",
    "        prior_var = variances[var_index]\n",
    "\n",
    "        #RBF width\n",
    "        l = ls[l_index]        \n",
    "        \n",
    "        print('## l   =',l)\n",
    "        print('## var =',prior_var)\n",
    "        \n",
    "        #Train test split\n",
    "        train_portion = 0.8\n",
    "        train_size = int(np.size(X,0)*train_portion)\n",
    "\n",
    "        #RBF centres\n",
    "        C = X[:train_size,:]\n",
    "\n",
    "        # #RBF train set\n",
    "        X_train = expand_inputs(l,X[:train_size,:],C)\n",
    "        X_train = np.column_stack([X_train,np.ones(X_train.shape[0])])\n",
    "        y_train = y[:train_size]\n",
    "\n",
    "        # #RBF test set\n",
    "        X_test = expand_inputs(l,X[train_size:,:],C)\n",
    "        X_test = np.column_stack([X_test,np.ones(X_test.shape[0])])\n",
    "        y_test = y[train_size:]\n",
    "        \n",
    "        #Initialise Gaussian prior\n",
    "        np.random.seed(42)\n",
    "        w_prior = np.random.normal(prior_mean,prior_var**0.5,X_train.shape[1])\n",
    "        w = w_prior\n",
    "        S_prior = prior_var * np.eye(w.size)\n",
    "\n",
    "        #MAP with bfgs\n",
    "        w_MAP = optimize.fmin_l_bfgs_b(neg_log_posterior_not_normalized, w_prior, fprime=grad_neg_log_posterior)[0]\n",
    "        #Calculate S_posterior\n",
    "        Hessian =  np.linalg.inv(S_prior)\n",
    "        Hessian += sum(logistic(np.dot(X_train[i],w_MAP)) * (1-logistic(np.dot(X_train[i],w_MAP))) * np.outer(X_train[i],X_train[i]) for i in range(X_train.shape[0]))\n",
    "        S_posterior = np.linalg.inv(Hessian)\n",
    "\n",
    "#         if np.all(np.linalg.eigvals(S_posterior) > 0): print('S_posterior is positive definite')\n",
    "        \n",
    "        plot_predictive_distribution(X,y,predict_for_full_bayesian,'Full Bayesian')\n",
    "        plot_predictive_distribution(X,y,predict_for_plot_expanded_features,'MAP')\n",
    "        \n",
    "        print('___FULL BAYESIAN___')\n",
    "        print('TRAIN LL:',compute_average_bayes_ll(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_bayes_ll(X_test,y_test,w_MAP,S_posterior))\n",
    "        \n",
    "        print('\\n___MAP___')\n",
    "        print('TRAIN LL:',compute_average_ll_fixed(X_train,y_train,w_MAP))\n",
    "#         print('TRAIN LOG MODEL EVIDENCE:',compute_average_lme(X_train,y_train,w_MAP,S_posterior))\n",
    "        print('TEST LL:',compute_average_ll_fixed(X_test,y_test,w_MAP))\n",
    "#         print('TEST LOG MODEL EVIDENCE:',compute_average_lme(X_test,y_test,w_MAP,S_posterior))\n",
    "\n",
    "        print('\\n___TEST MODEL EVIDENCE___:')\n",
    "        grid3[l_index,var_index] = compute_average_lme(X_test,y_test,w_MAP,S_posterior,prior_var)\n",
    "        \n",
    "        TP, TN, FP, FN = 0,0,0,0\n",
    "        for i in range(len(X_test)):\n",
    "            mu_a = np.dot(w_MAP, X_test[i,:])\n",
    "            sigma_a = np.dot(X_test[i,:],np.dot(S_posterior,X_test[i,:]))\n",
    "            kappa = (1 + np.pi*sigma_a/8)**-0.5\n",
    "            \n",
    "            prediction = 1 if logistic(kappa*mu_a)>0.5 else 0\n",
    "            if prediction == y_test[i]:\n",
    "                if y_test[i]==0: TN += 1\n",
    "                elif y_test[i]==1: TP += 1\n",
    "            else:\n",
    "                if y_test[i]==0: FP += 1\n",
    "                elif y_test[i]==1: FN += 1\n",
    "        print('===CONFUSION MATRIX===')\n",
    "        print(TN/(TN+FP), FP/(TN+FP))\n",
    "        print(FN/(FN+TP), TP/(FN+TP))\n",
    "        \n",
    "        print(\"______________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.980405200Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_df = pd.DataFrame(data = np.round(grid3,2), index=np.round(ls,2), columns=np.round(variances,2))\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.981461600Z"
    }
   },
   "outputs": [],
   "source": [
    "grid3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.981461600Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_df.to_latex('grid3.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-03-08T12:54:46.982472Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

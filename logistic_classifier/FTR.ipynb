{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = np.loadtxt('X.txt')\n",
    "y = np.loadtxt('y.txt')\n",
    "\n",
    "# We randomly permute the data\n",
    "permutation = np.random.permutation(X.shape[ 0 ])\n",
    "X = X[ permutation, : ]\n",
    "y = y[ permutation ]\n",
    "\n",
    "n_train = 800\n",
    "X_train = X[ 0 : n_train, : ]\n",
    "X_test = X[ n_train :, : ]\n",
    "y_train = y[ 0 : n_train ]\n",
    "y_test = y[ n_train : ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Short Lab Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_data_internal(X, y):\n",
    "    x_min, x_max = X[ :, 0 ].min() - .5, X[ :, 0 ].max() + .5\n",
    "    y_min, y_max = X[ :, 1 ].min() - .5, X[ :, 1 ].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    plt.figure()\n",
    "    plt.xlim(xx.min(None), xx.max(None))\n",
    "    plt.ylim(yy.min(None), yy.max(None))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(X[y == 0, 0], X[y == 0, 1], 'ro', label = 'Class 1')\n",
    "    ax.plot(X[y == 1, 0], X[y == 1, 1], 'bo', label = 'Class 2')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title('Plot data')\n",
    "    plt.legend(loc = 'upper left', scatterpoints = 1, numpoints = 1)\n",
    "    return xx, yy\n",
    "\n",
    "def plot_data(X, y):\n",
    "    xx, yy = plot_data_internal(X, y)\n",
    "    plt.show()\n",
    "\n",
    "def logistic(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def predict(X_tilde, w): return logistic(np.dot(X_tilde, w))\n",
    "\n",
    "def compute_average_ll(X_tilde, y, w):\n",
    "    output_prob = predict(X_tilde, w)\n",
    "    return np.mean(y * np.log(output_prob) + (1 - y) * np.log(1.0 - output_prob))\n",
    "\n",
    "def get_x_tilde(X): return np.concatenate((np.ones((X.shape[ 0 ], 1 )), X), 1)\n",
    "\n",
    "\n",
    "def fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha):\n",
    "    # Setup table\n",
    "    table = PrettyTable()\n",
    "    table.title=\"train/test log likelihood\"\n",
    "    table.field_names=[\"# it.\", \"train loss\", \"test loss\"]\n",
    "\n",
    "    w = np.random.randn(X_tilde_train.shape[ 1 ])\n",
    "    #print(f\"Shape o w: {w.shape}\")\n",
    "    ll_train = np.zeros(n_steps)\n",
    "    ll_test = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        sigmoid_value = predict(X_tilde_train, w)\n",
    "        grad_ll = (y_train - sigmoid_value).T @ X_tilde_train\n",
    "        #print(f\"Grad ll shape:{grad_ll.shape}\")\n",
    "        w = w + alpha * grad_ll # Gradient-based update rule for w.\n",
    "        ll_train[ i ] = compute_average_ll(X_tilde_train, y_train, w)\n",
    "        ll_test[ i ] = compute_average_ll(X_tilde_test, y_test, w)\n",
    "        table.add_row([i, ll_train[i], ll_test[i]])\n",
    "    print(table)\n",
    "    return w, ll_train, ll_test\n",
    "\n",
    "def plot_ll(ll):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.xlim(0, len(ll) + 2)\n",
    "    plt.ylim(min(ll) - 0.1, max(ll) + 0.1)\n",
    "    ax.plot(np.arange(1, len(ll) + 1), ll, 'r-')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average log-likelihood')\n",
    "    plt.title('Plot Average Log-likelihood Curve')\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictive_distribution(X, y, w, map_inputs = lambda x : x):\n",
    "    xx, yy = plot_data_internal(X, y)\n",
    "    ax = plt.gca()\n",
    "    X_tilde = get_x_tilde(map_inputs(np.concatenate((xx.ravel().reshape((-1, 1)), yy.ravel().reshape((-1, 1))), 1)))\n",
    "    Z = predict(X_tilde, w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs2 = ax.contour(xx, yy, Z, cmap = 'RdBu', linewidths = 2)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize = 14)\n",
    "    plt.show()\n",
    "\n",
    "def get_confusion_matrix(X,y,w,tau=0.5):\n",
    "    X_tilde = get_x_tilde(X)\n",
    "    pred_soft = predict(X_tilde,w)\n",
    "    y_hat = (pred_soft>tau)\n",
    "    TP = np.count_nonzero(y_hat[y==1])\n",
    "    FN = y_hat[y==1].shape[0] - TP\n",
    "    FP = np.count_nonzero(y_hat[y == 0])\n",
    "    TN = y_hat[y == 0].shape[0] - FP\n",
    "\n",
    "    return np.array(\n",
    "        [[TN/(TN+FP),FP/(TN+FP)],\n",
    "         [FN/(TP+FN),TP/(TP+FN)]]\n",
    "    )\n",
    "\n",
    "def evaluate_basis_functions(l, X, Z):\n",
    "    X2 = np.sum(X**2, 1)\n",
    "    Z2 = np.sum(Z**2, 1)\n",
    "    ones_Z = np.ones(Z.shape[ 0 ])\n",
    "    ones_X = np.ones(X.shape[ 0 ])\n",
    "    r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)\n",
    "    return np.exp(-0.5 / l**2 * r2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_data(X, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We train the classifier\n",
    "alpha = 0.001\n",
    "n_steps = 40\n",
    "\n",
    "X_tilde_train = get_x_tilde(X_train)\n",
    "X_tilde_test = get_x_tilde(X_test)\n",
    "\n",
    "w, ll_train, ll_test = fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "plot_ll(ll_train)\n",
    "plot_ll(ll_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_predictive_distribution(X, y, w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mtx = get_confusion_matrix(X_test,y_test,w)\n",
    "print(f\"Confussion matrix: {mtx}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# We expand the data\n",
    "\n",
    "l = 0.01 # Width of the Gaussian basis function.\n",
    "\n",
    "X_tilde_train = get_x_tilde(evaluate_basis_functions(l, X_train, X_train))\n",
    "X_tilde_test = get_x_tilde(evaluate_basis_functions(l, X_test, X_train))\n",
    "\n",
    "# We train the new classifier on the feature expanded inputs\n",
    "\n",
    "alpha = 0.008 # Learning rate for gradient-based optimisation with basis functions.\n",
    "n_steps = 5_000 # Number of steps of gradient-based optimisation with basis functions.\n",
    "\n",
    "w, ll_train, ll_test = fit_w( X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha)\n",
    "\n",
    "# We plot the training and test log likelihoods\n",
    "\n",
    "plot_ll(ll_train)\n",
    "plot_ll(ll_test)\n",
    "\n",
    "# We plot the predictive distribution\n",
    "\n",
    "plot_predictive_distribution(X, y, w, lambda x : evaluate_basis_functions(l, x, X_train))\n",
    "\n",
    "mtx = get_confusion_matrix(evaluate_basis_functions(l, X_test, X_train),y_test,w)\n",
    "print(f\"Confusion matrix: {mtx}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_hessian(\n",
    "        w:NDArray,\n",
    "        X_tilde:NDArray, # Use Phi here if using RBFs\n",
    "        sigma_0:int = 1\n",
    "):\n",
    "    sigmoid_value = predict(X_tilde,w)\n",
    "    v = sigmoid_value * (1- sigmoid_value) # Hadamard prod\n",
    "    A =  X_tilde.T @ np.diag(v) @ X_tilde\n",
    "    A = A + np.ones_like(A) / sigma_0 **2\n",
    "    return A\n",
    "\n",
    "\n",
    "def evaluate_objective(\n",
    "        w:NDArray,\n",
    "        X_tilde:NDArray,\n",
    "        y:NDArray,\n",
    "        sigma_0:float = 1.0\n",
    "):\n",
    "    log_sigmoid = np.log( predict(X_tilde,w) )\n",
    "    return - (1/sigma_0**2) * np.dot(w,w) + np.dot(y, log_sigmoid) + np.dot( np.ones_like(y)-y, log_sigmoid)\n",
    "\n",
    "def get_objective(\n",
    "        X_tilde:NDArray,\n",
    "        y:NDArray,\n",
    "        sigma_0:float = 1.0\n",
    "):\n",
    "    return lambda w : evaluate_objective(w,X_tilde,y,sigma_0)\n",
    "\n",
    "def evaluate_jacobian(\n",
    "    w:NDArray,\n",
    "    X_tilde: NDArray,\n",
    "    y: NDArray,\n",
    "    sigma_0: float = 1.0\n",
    "):\n",
    "    sigmoid_value = predict(X_tilde,w)\n",
    "    return w / sigma_0 ** 2 - (y - sigmoid_value).T @ X_tilde\n",
    "\n",
    "def get_jacobian(\n",
    "        X_tilde: NDArray,\n",
    "        y: NDArray,\n",
    "        sigma_0: float = 1.0\n",
    "):\n",
    "    return lambda w : evaluate_jacobian(w,X_tilde,y,sigma_0)\n",
    "\n",
    "def optimize(\n",
    "        X_tilde: NDArray,\n",
    "        y: NDArray,\n",
    "        sigma_0: float = 1.0\n",
    "): # L-BFGS-B algo to compute the MAP estimator for w  && Start optimization at origin\n",
    "    J = get_objective(X_tilde,y,sigma_0)\n",
    "    dJ = get_jacobian(X_tilde,y,sigma_0)\n",
    "    w_hat, J_min, d = fmin_l_bfgs_b( func = J, x0 =np.zeros(X_tilde.shape[1]), fprime = dJ)\n",
    "    table = PrettyTable()\n",
    "    table.title=\"Optimization\"\n",
    "    table.field_names=[\"w_hat\",\"J_min\",\"warnflag\", \"grad\", \"nit\"]\n",
    "    table.add_row([w_hat,J_min,d[\"warnflag\"], d[\"grad\"], d[\"nit\"]])\n",
    "    print(table)\n",
    "    return w_hat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Laplace Approximation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    N= 30\n",
    "    M= 2\n",
    "    w = np.ones(1+M)\n",
    "    X = np.ones([N,M])\n",
    "    X_tilde =get_x_tilde(X)\n",
    "    A = get_hessian(w,X_tilde)\n",
    "    print(A)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

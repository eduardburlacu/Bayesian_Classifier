{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-27T21:20:30.087566900Z",
     "start_time": "2024-01-27T21:20:29.695940100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# We load the data\n",
    "X = np.loadtxt('../logistic_classifier/X.txt')\n",
    "y = np.loadtxt('../logistic_classifier/y.txt')\n",
    "\n",
    "# We randomly permute the data\n",
    "permutation = np.random.permutation(X.shape[ 0 ])\n",
    "X = X[ permutation, : ]\n",
    "y = y[ permutation ]\n",
    "\n",
    "# We split the data into train and test sets\n",
    "\n",
    "n_train = 800\n",
    "X_train = X[ 0 : n_train, : ]\n",
    "X_test = X[ n_train :, : ]\n",
    "y_train = y[ 0 : n_train ]\n",
    "y_test = y[ n_train : ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T21:20:30.091419800Z",
     "start_time": "2024-01-27T21:20:30.091419800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "##\n",
    "# Function that plots the points in 2D together with their labels\n",
    "#\n",
    "# Inputs:\n",
    "#\n",
    "# X: 2d array with the input features\n",
    "# y: 1d array with the class labels (0 or 1)\n",
    "#\n",
    "# Output: 2D matrices with the x and y coordinates of the points shown in the plot\n",
    "#\n",
    "\n",
    "def plot_data_internal(X, y):\n",
    "    x_min, x_max = X[ :, 0 ].min() - .5, X[ :, 0 ].max() + .5\n",
    "    y_min, y_max = X[ :, 1 ].min() - .5, X[ :, 1 ].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    plt.figure()\n",
    "    plt.xlim(xx.min(None), xx.max(None))\n",
    "    plt.ylim(yy.min(None), yy.max(None))\n",
    "    ax = plt.gca()\n",
    "    ax.plot(X[y == 0, 0], X[y == 0, 1], 'ro', label = 'Class 1')\n",
    "    ax.plot(X[y == 1, 0], X[y == 1, 1], 'bo', label = 'Class 2')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.title('Plot data')\n",
    "    plt.legend(loc = 'upper left', scatterpoints = 1, numpoints = 1)\n",
    "    return xx, yy\n",
    "\n",
    "##\n",
    "# Function that plots the data without returning anything by calling \"plot_data_internal\".\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# X: 2d array with the input features\n",
    "# y: 1d array with the class labels (0 or 1)\n",
    "#\n",
    "# Output: Nothing.\n",
    "#\n",
    "\n",
    "def plot_data(X, y):\n",
    "    xx, yy = plot_data_internal(X, y)\n",
    "    plt.show()\n",
    "\n",
    "# The logistic function\n",
    "\n",
    "def logistic(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "##\n",
    "# Function that makes predictions with a logistic classifier\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# X_tile: matrix of input features (with a constant 1 appended to the left)\n",
    "#         for which to make predictions\n",
    "# w: vector of model parameters\n",
    "#\n",
    "# Output: The predictions of the logistic classifier\n",
    "#\n",
    "\n",
    "def predict(X_tilde, w): return logistic(np.dot(X_tilde, w))\n",
    "\n",
    "##\n",
    "# Function that computes the average loglikelihood of the logistic classifier on some data.\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# X_tile: matrix of input features (with a constant 1 appended to the left)\n",
    "#         for which to make predictions\n",
    "# y: vector of binary output labels\n",
    "# w: vector of model parameters\n",
    "#\n",
    "# Output: The average loglikelihood\n",
    "#\n",
    "\n",
    "def compute_average_ll(X_tilde, y, w):\n",
    "    output_prob = predict(X_tilde, w)\n",
    "    return np.mean(y * np.log(output_prob) + (1 - y) * np.log(1.0 - output_prob))\n",
    "\n",
    "##\n",
    "# Function that expands a matrix of input features by adding a column equal to 1.\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# X: matrix of input features.\n",
    "#\n",
    "# Output: Matrix x_tilde with one additional constant column equal to 1 added.\n",
    "#\n",
    "\n",
    "def get_x_tilde(X): return np.concatenate((np.ones((X.shape[ 0 ], 1 )), X), 1)\n",
    "\n",
    "##\n",
    "# Function that finds the model parameters by optimising the likelihood using gradient descent\n",
    "#\n",
    "# Input:\n",
    "#\n",
    "# X_tile_train: matrix of training input features (with a constant 1 appended to the left)\n",
    "# y_train: vector of training binary output labels\n",
    "# X_tile_test: matrix of test input features (with a constant 1 appended to the left)\n",
    "# y_test: vector of test binary output labels\n",
    "# alpha: step_size_parameter for the gradient based optimisation\n",
    "# n_steps: the number of steps of gradient based optimisation\n",
    "#\n",
    "# Output:\n",
    "#\n",
    "# 1 - Vector of model parameters w\n",
    "# 2 - Vector with average log-likelihood values obtained on the training set\n",
    "# 3 - Vector with average log-likelihood values obtained on the test set\n",
    "#\n",
    "\n",
    "def fit_w(X_tilde_train, y_train, X_tilde_test, y_test, n_steps, alpha):\n",
    "    # Setup table\n",
    "    table = PrettyTable()\n",
    "    table.title=\"train/test log likelihood\"\n",
    "    table.field_names=[\"# it.\", \"train loss\", \"test loss\"]\n",
    "\n",
    "    w = np.random.randn(X_tilde_train.shape[ 1 ])\n",
    "    ll_train = np.zeros(n_steps)\n",
    "    ll_test = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        sigmoid_value = predict(X_tilde_train, w)\n",
    "        grad_ll = (y_train - sigmoid_value).T @ X_tilde_train\n",
    "        w = w + alpha * grad_ll # Gradient-based update rule for w.\n",
    "        ll_train[ i ] = compute_average_ll(X_tilde_train, y_train, w)\n",
    "        ll_test[ i ] = compute_average_ll(X_tilde_test, y_test, w)\n",
    "        table.add_row([i, ll_train[i], ll_test[i]])\n",
    "    print(table)\n",
    "    return w, ll_train, ll_test\n",
    "\n",
    "def plot_ll(ll):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    plt.xlim(0, len(ll) + 2)\n",
    "    plt.ylim(min(ll) - 0.1, max(ll) + 0.1)\n",
    "    ax.plot(np.arange(1, len(ll) + 1), ll, 'r-')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average log-likelihood')\n",
    "    plt.title('Plot Average Log-likelihood Curve')\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictive_distribution(X, y, w, map_inputs = lambda x : x):\n",
    "    xx, yy = plot_data_internal(X, y)\n",
    "    ax = plt.gca()\n",
    "    X_tilde = get_x_tilde(map_inputs(np.concatenate((xx.ravel().reshape((-1, 1)), yy.ravel().reshape((-1, 1))), 1)))\n",
    "    Z = predict(X_tilde, w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs2 = ax.contour(xx, yy, Z, cmap = 'RdBu', linewidths = 2)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize = 14)\n",
    "    plt.show()\n",
    "\n",
    "def get_confusion_matrix(X,y,w,tau=0.5):\n",
    "    X_tilde = get_x_tilde(X)\n",
    "    pred_soft = predict(X_tilde,w)\n",
    "    y_hat = (pred_soft>tau)\n",
    "    TP = np.count_nonzero(y_hat[y==1])\n",
    "    FN = y_hat[y==1].shape[0] - TP\n",
    "    FP = np.count_nonzero(y_hat[y == 0])\n",
    "    TN = y_hat[y == 0].shape[0] - FP\n",
    "    return np.array(\n",
    "        [[TN/(TN+FP),FP/(TN+FP)],\n",
    "         [FN/(TP+FN),TP/(TP+FN)]]\n",
    "    )\n",
    "\n",
    "# Function that replaces initial input features by evaluating Gaussian basis functions\n",
    "# on a grid of points\n",
    "#\n",
    "# Inputs:\n",
    "#\n",
    "# l: hyperparameter for the width of the Gaussian basis functions\n",
    "# Z: location of the Gaussian basis functions\n",
    "# X: points at which to evaluate the basis functions\n",
    "#\n",
    "# Output: Feature matrix with the evaluations of the Gaussian basis functions.\n",
    "#\n",
    "\n",
    "def evaluate_basis_functions(l, X, Z):\n",
    "    X2 = np.sum(X**2, 1)\n",
    "    Z2 = np.sum(Z**2, 1)\n",
    "    ones_Z = np.ones(Z.shape[ 0 ])\n",
    "    ones_X = np.ones(X.shape[ 0 ])\n",
    "    r2 = np.outer(X2, ones_Z) - 2 * np.dot(X, Z.T) + np.outer(ones_X, Z2)\n",
    "    return np.exp(-0.5 / l**2 * r2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T21:20:30.091419800Z",
     "start_time": "2024-01-27T21:20:30.091419800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T21:20:30.134999800Z",
     "start_time": "2024-01-27T21:20:30.091419800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
